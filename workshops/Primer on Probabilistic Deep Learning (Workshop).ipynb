{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-mercy",
   "metadata": {},
   "source": [
    "# 1. Introductory Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-needle",
   "metadata": {},
   "source": [
    "<b>Distribution objects</b> capture the essencial operations on probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-batch",
   "metadata": {},
   "source": [
    "## 1.1 Univariate Distributions\n",
    "\n",
    "Univariate distributions are distribution of a single random variable.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathcal{N} &= (\\mu,\\sigma) \\\\\n",
    "\\mu &= 0 \\\\\n",
    "\\sigma &= 1 \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a univariate normal distribution with μ=0 and σ=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-pixel",
   "metadata": {},
   "source": [
    "Notice the properties `batch_shape` and `event_shape`. The `event_shape` property is what captures the dimensionality of the random variable. In this case we are using a univariate distribution, that is why the `event_shape` is empty. \n",
    "\n",
    "What can we do with this distribution object. We can, for instance, sample from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw one sample from the normal distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw 3 samples from the normal distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-weekend",
   "metadata": {},
   "source": [
    "We can also evaluate the Probability Density Function (PDF), in the case of continuous random variables, on a given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pdf(0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-jerusalem",
   "metadata": {},
   "source": [
    "We will usually use the log probability of a given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the log_prob(0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is simply the log of the value returned by the prob method \n",
    "\n",
    "np.log(normal.prob(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram, approximating the density of the distribution\n",
    "\n",
    "plt.hist(normal.sample(10000).numpy(), bins=50, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-graphics",
   "metadata": {},
   "source": [
    "We can also represent in a single object a batch of distribution of the same type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two univariate normal distributions:\n",
    "# one with μ=0 and σ=1\n",
    "# and another with μ=1 and σ=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-glenn",
   "metadata": {},
   "source": [
    "Notice the batch shape in the object above, it has a batch shape of two, meaning that we have two different normal distributions represented in that object. We can obviously sample from this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the batch 2 normal distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-taiwan",
   "metadata": {},
   "source": [
    "Now, the shape of our samples is (3,2), as we are drawing 3 samples from each normal distribution. The same way we can get the values for the PDFs of both distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the PDF values for the batch distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-journalism",
   "metadata": {},
   "source": [
    "The returned shape is the original batch shape. We can go crazy with dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a batch shape with higher rank\n",
    "\n",
    "normal_batch_nD = tfd.Normal([[[0., 1],\n",
    "                             [0.5, 1],\n",
    "                             [3, 2]]], scale=1)\n",
    "normal_batch_nD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the batch of distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_batch_nD.sample(10).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-punishment",
   "metadata": {},
   "source": [
    "The shape is now (10, 1, 3, 2), where the first dimension is the sample size and the rest are the batched distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-pavilion",
   "metadata": {},
   "source": [
    "## 1.2 Multivariate Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-expense",
   "metadata": {},
   "source": [
    "Let's start by defining a multivariate normal distribution. There are multiple ways to do it that you can check in the documentation. It is defined as:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathcal{N_1} &= (\\mu_1,\\Sigma_1) \\\\\n",
    "\\mu_1 &= [0, 1] \\\\\n",
    "\\Sigma_1 &= \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multivariate normal distribution with diagonal covariance\n",
    "# with μ = [0, 1] and σ = [1, 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-bandwidth",
   "metadata": {},
   "source": [
    "Notice the difference from the distributions that we have created above. The `event_shape` is now 2, indicating that the random variable for this distribution is 2-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the multivariate normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the samples of the distribution\n",
    "\n",
    "plt_sample = mv_normal.sample(10000).numpy()\n",
    "plt.scatter(plt_sample[:,0], plt_sample[:,1], marker='.', alpha=0.05)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-commodity",
   "metadata": {},
   "source": [
    "The data is more spread out in the second dimension (y-axis) as we defined a scale that is twice the value used for the first dimension. Also, there is no correlation between the dimensions, as we used a diagonal multivariate normal distribution (off-diagonal terms are 0).\n",
    "\n",
    "We can define a batch of multivariate distributions.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathcal{N_1} &= (\\mu_1,\\Sigma_1) \\\\\n",
    "\\mu_1 &= [0, 0] \\\\\n",
    "\\Sigma_1 &= \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 2\n",
    "\\end{bmatrix} \\\\[10pt]\n",
    "\\mathcal{N_2} &= (\\mu_2,\\Sigma_2) \\\\\n",
    "\\mu_1 &= [1, 1] \\\\\n",
    "\\Sigma_1 &= \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix} \\\\[10pt]\n",
    "\\mathcal{N_3} &= (\\mu_3,\\Sigma_3) \\\\\n",
    "\\mu_1 &= [0, 0] \\\\\n",
    "\\Sigma_1 &= \\begin{bmatrix}\n",
    "2 & 0\\\\\n",
    "0 & 10\n",
    "\\end{bmatrix}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 batches of multivariate normals\n",
    "\n",
    "normal_diag_batch = tfd.MultivariateNormalDiag(loc=[[0,0], [1,1], [0,0]],\n",
    "                                              scale_diag=[[1,2], [1,1], [2,10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-onion",
   "metadata": {},
   "source": [
    "When sampling we get an output with the following dimensions (number_samples, batch_shape, event_shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-algebra",
   "metadata": {},
   "source": [
    "The log-probability has shape (number_samples, batch_shape) as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample for a plot\n",
    "\n",
    "plt_sample_batch = normal_diag_batch.sample(10000).numpy()\n",
    "plt_sample_batch.shape # (samples, batch, event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the samples of the mv normal\n",
    "\n",
    "fig, axs = (plt.subplots(1, 3, sharex=True, sharey=True, figsize=(10, 3)))\n",
    "titles = ['cov_diag=[1,2]', 'cov_diag=[1,1]', f'cov_diag=[2,10]']\n",
    "\n",
    "for i, (ax, title) in enumerate(zip(axs, titles)):\n",
    "    samples = plt_sample_batch[:,i,:] # take the ith batch, gives a shape (samples, event_shape)\n",
    "    ax.scatter(samples[:,0], samples[:,1], marker='.', alpha=0.05)\n",
    "    ax.set_title(title)\n",
    "    axs[i].set_ylim(-25, 25)\n",
    "    axs[i].set_xlim(-25, 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-toronto",
   "metadata": {},
   "source": [
    "## 1.3 Independent Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-gregory",
   "metadata": {},
   "source": [
    "There are cases where we want to interpret a batch of independent distributions over an event space as a single joint distribution over a product of event spaces. This impacts the way we handle the batch and event shapes. An example of such a problem is the Naive Bayes classifier, where the features are independent given a class label.\n",
    "\n",
    "To illustrate, let's define two normal distributions.\n",
    "The first is a multivariate normal of the form:\n",
    "$$\\begin{align}\n",
    "\\mathcal{N_1} &= (\\mu_1,\\Sigma_1) \\\\\n",
    "\\mu_1 &= [0, 1] \\\\\n",
    "\\Sigma_1 &= \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "The second is a batched normal of the form:\n",
    "$$\\begin{align}\n",
    "\\mathcal{N_{2_1}} &= (\\mu_{2_1},\\sigma_{2_1}) \\\\\n",
    "\\mu_{2_1} &= 0 \\\\\n",
    "\\sigma_{2_1} &= 1 \\\\[10pt]\n",
    "\\mathcal{N_{2_2}} &= (\\mu_{2_2},\\sigma_{2_2}) \\\\\n",
    "\\mu_{2_2} &= 1 \\\\\n",
    "\\sigma_{2_2} &= 2\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multivariate normal diagonal distribution\n",
    "# with μ=[0,1], σ=[1, 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the independent dist\n",
    "\n",
    "samples = mv_normal.sample(10000).numpy()\n",
    "x1 = samples[:,0]\n",
    "x2 = samples[:,1]\n",
    "sns.jointplot(x = x1, y = x2, kind='kde', space=0, color='b', xlim=[-6, 7], ylim=[-6, 7]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batched normal distribution\n",
    "# with μ=[0,1], σ=[1, 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the batched normal\n",
    "\n",
    "t = np.linspace(-4, 4, 10000)\n",
    "densities = batched_normal.prob(np.repeat(t[:, np.newaxis], 2, axis=1))\n",
    "\n",
    "sns.lineplot(x=t, y=densities[:, 0], label='loc={}, scale={}'.format(locs[0], scales[0]))\n",
    "sns.lineplot(x=t, y=densities[:, 1], label='loc={}, scale={}'.format(locs[1], scales[1]))\n",
    "plt.ylabel('Probability Density')\n",
    "plt.xlabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-provincial",
   "metadata": {},
   "source": [
    "Notice that the first distribution object returns a single log-probability while the second returns 2. The difference is that the array that we pass to the first is interpreted as a single realization of a 2-dimensional random variable. In the second case, the array is interpreted as different inputs for each of the random variables (also referred as batches).\n",
    "\n",
    "In a nutshell the Independent distribution allows us to absorb whatever dimesions we want to absorb to the event dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an independent normal dist from the batched normal dist created above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the independent dist\n",
    "\n",
    "samples = independent_normal.sample(10000).numpy()\n",
    "x1 = samples[:,0]\n",
    "x2 = samples[:,1]\n",
    "sns.jointplot(x = x1, y = x2, kind='kde', space=0, color='b', xlim=[-6, 7], ylim=[-6, 7]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-female",
   "metadata": {},
   "source": [
    "Notice that now the independent normal created from the batched normal is equivalent to the multivariate normal created above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-oregon",
   "metadata": {},
   "source": [
    "## 1.4 Trainable Parameters\n",
    "\n",
    "Now that we know the essencials about TensorFlow Probability objects, it is time to understand how we can train parameters for these distributions. This is the connection that we are missing to start applying what we have learned. \n",
    "\n",
    "In TensorFlow, `Variable` objects are what we use to capture the values of parameter of our deep learning models. These objects are updated during training by, for example, applying gradients obtained from a loss function and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a normal distribution with the μ parameter trainable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-finish",
   "metadata": {},
   "source": [
    "For the training procedure, Maximum Likelihood is the usual suspect in deep learning models. In a nutshell, we are looking for the parameters of our model that maximize the probability of the data. \n",
    "\n",
    "The probability density function of a continuous random variable roughly indicates the probability of a sample taking a particular value. We will denote this function $P(x | \\theta)$ where $x$ is the value of the sample and $\\theta$ is the parameter describing the probability distribution:\n",
    "\n",
    "$$\n",
    "P(x | \\theta) = \\text{Prob} (\\text{sampling value $x$ from a distribution with parameter $\\theta$}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pdf of 0 for a standard normal dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-allen",
   "metadata": {},
   "source": [
    "When more than one sample is drawn *independently* from the same distribution (which we usually assume), the probability density function of the sample values $x_1, \\ldots, x_n$ is the product of the probability density functions for each individual $x_i$. Written formally:\n",
    "\n",
    "$$\n",
    "P(x_1, \\ldots, x_n | \\theta) = \\prod_{i=1}^n P(x_i | \\theta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [-0.5, 0, 1.5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-jewel",
   "metadata": {},
   "source": [
    "Probability density functions are usually considered functions of $x_1, \\ldots, x_n$, with the parameter $\\theta$ considered fixed. They are used when you know the parameter $\\theta$ and want to know the probability of a sample taking some values $x_1, \\ldots, x_n$. You use this function in *probability*, where you know the distribution and want to make deductions about possible values sampled from it.\n",
    "\n",
    "The *likelihood* function is the same, but with the $x_1, \\ldots, x_n$ considered fixed and with $\\theta$ considered the independent variable. You usually use this function when you know the sample values $x_1, \\ldots, x_n$ (because you've observed them by collecting data), but don't know the parameter $\\theta$. You use this function in *statistics*, where you know the data and want to make inferences about the distribution they came from. \n",
    "\n",
    "For the likelihood, the convention is using the letter $L$, so that\n",
    "\n",
    "$$\n",
    "\\underbrace{L(x_1, \\ldots, x_n | \\theta)}_{\\text{ likelihood,} \\\\ \\text{function of $\\theta$}} = \\underbrace{P(x_1, \\ldots, x_n | \\theta)}_{\\text{probabiliy density,} \\\\ \\text{ function of $x_1, \\ldots, x_n$}}\n",
    "$$\n",
    "\n",
    "We are ready to define our likelihood function for a normal distribution with parameters $\\mu$ and $\\sigma$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "L = f(X|\\theta) &= f(x_1|\\theta) f(x_2|\\theta),..., f(x_n|\\theta) \\\\\n",
    "&= \\prod^n_{j=1}f(X| \\mu,\\sigma^2) \\\\\n",
    "&= (2\\pi\\sigma^2)^{-n/2} \\exp{\\big(-\\frac{1}{2\\sigma^2} \\sum^n_{j=1}(x_i-\\mu)^2\\big)}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of values for μ and σ and calculate the likelihood\n",
    "# for the different parameter values\n",
    "\n",
    "μ = np.linspace(-2, 2, 100)\n",
    "σ = np.linspace(0, 3, 100)\n",
    "\n",
    "l_x = []\n",
    "for mu in μ:\n",
    "    for sigma in σ:\n",
    "        l_x.append(np.prod(tfd.Normal(mu, sigma).prob(X)))\n",
    "        \n",
    "l_x = np.asarray(l_x).reshape((100, 100)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the likelihood based on the observed data (fixed)\n",
    "\n",
    "plt.contourf(μ, σ, l_x)\n",
    "plt.xlabel('μ')\n",
    "plt.ylabel('σ')\n",
    "plt.colorbar()\n",
    "plt.title('Likelihood');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-great",
   "metadata": {},
   "source": [
    "We now get to a new problem, multiplying many small probabilities together can be numerically unstable. To overcome this, we can use the log of the same function. The natural logarithm is a monotonically increasing function, which means that if the value on the x-axis increases, the value on the y-axis also increases. This is important because it ensures that the maximum value of the log of the probability occurs at the same point as the original probability function. It does another very conveniently thing for us, it transforms our products into sums.\n",
    "\n",
    "Let's perform the transformation:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\log(L(X|\\theta)) &= \\log\\big((2\\pi\\sigma^2)^{-n/2} \\exp{\\big(-\\frac{1}{2\\sigma^2} \\sum^n_{j=1}(y_i-\\mu)^2\\big)\\big)} \\\\\n",
    "&= -\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{j=1}^{n}(y_i-\\mu)^2\n",
    "\\end{aligned}$$\n",
    "\n",
    "Almost there! We will now work our optimization problem at hand. We can put it as simple as\n",
    "\n",
    "$$\\max_{\\mu,\\sigma^2}\\log(L(X|\\theta))$$\n",
    "\n",
    "The expression derived above can be differentiated to find the maximum. Expanding our parameters we have $\\log(L(X|\\mu, \\sigma))$. As it is a function of the two variables $\\mu$ and $\\sigma$ we use partial derivatives to find the MLE. \n",
    "\n",
    "Let's focus on $\\hat \\mu$ (the hat indicates that it is an estimator, i.e. our output), we compute it from\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& \\quad \\frac{\\partial}{\\partial \\mu} \\log(L(Y|\\mu, \\sigma)) \\\\\n",
    "&= \\frac{\\partial}{\\partial \\mu} \\big(-\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{j=1}^{n}(x_i-\\mu)^2\\big)\n",
    "\\\\\n",
    "&= \\sum^n_{j=1} \\frac{(x_i - \\mu)}{\\sigma^2}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Setting the expression above equal to zero we get\n",
    "\n",
    "\n",
    "$$\\sum^n_{j=1} \\frac{(x_i - \\mu)}{\\sigma^2} = 0 $$\n",
    "\n",
    "Then\n",
    "$$\\begin{aligned}\n",
    "\\hat\\mu &= \\frac{\\sum^n_{j=1}x_i}{n} \\\\\n",
    "\\hat\\mu &= \\bar x\n",
    "\\end{aligned}$$\n",
    "\n",
    "Surprisingly or not this is the mean of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maximum values for the μ and σ and compare with true values\n",
    "\n",
    "index_mu_max = np.argmax(l_x, axis=1)[-1]\n",
    "print(f'μ True Value: {np.array(X).mean()}')\n",
    "print(f'μ Calculated Value: {μ[index_mu_max]}')\n",
    "print(f'σ True Value: {np.array(X).std()}')\n",
    "print(f'σ Calculated Value: {σ[np.nanargmax(l_x[:,index_mu_max], axis=0)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-supply",
   "metadata": {},
   "source": [
    "##### Calculate using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random variable normall distributed and sample from it\n",
    "\n",
    "x_train = np.random.normal(loc=1, scale=5, size=1000).astype('float32')[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the random variable\n",
    "\n",
    "plt.hist(x_train, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the samples of the random variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the negative log likelihood function (loss function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-database",
   "metadata": {},
   "source": [
    "Notice that above we are supplying data points to our model and computing the corresponding log-probability for each data point. As we already saw, the `log_prob` method returns a tensor that has the same shape as the data. The log-probability of our data will be the sum of the log-probabilities of each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training loop\n",
    "\n",
    "@tf.function\n",
    "def get_loss_and_grads(x_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(normal.trainable_variables)\n",
    "        loss = nll(x_train)\n",
    "        grads = tape.gradient(loss, normal.trainable_variables)\n",
    "    return loss, grads\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "for i in range(2000):\n",
    "    loss, grads = get_loss_and_grads(x_train)\n",
    "    optimizer.apply_gradients(zip(grads, normal.trainable_variables))\n",
    "    \n",
    "    loc_value = normal.loc.value()\n",
    "    print(\"Step {:03d}: Loss: {:.3f} Loc: {:.3f}\".format(i, loss, loc_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the true value and the estimated parameter\n",
    "\n",
    "print(f'True Value: {x_train.mean()}')\n",
    "print(f'Estimated Value: {normal.trainable_variables[0].numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-tonight",
   "metadata": {},
   "source": [
    "# 2. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-devil",
   "metadata": {},
   "source": [
    "#### Create data\n",
    "\n",
    "The data we'll be working with is artificially created from the following equation:\n",
    "\n",
    "$$y_i = x_i + \\frac{3}{10}\\epsilon_i$$\n",
    "\n",
    "where $\\epsilon_i \\sim \\mathcal{N}(0,1)$ are independent and identically distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and plot 100 points of training data\n",
    "\n",
    "x_train = np.linspace(-1, 1, 100)[:, np.newaxis]\n",
    "y_train = x_train + 0.3*np.random.randn(100)[:, np.newaxis]\n",
    "\n",
    "plt.scatter(x_train, y_train, alpha=0.4)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-thriller",
   "metadata": {},
   "source": [
    "#### Deterministic linear regression with MSE loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-duplicate",
   "metadata": {},
   "source": [
    "Let's define a model that receives each input as a length one vector (`input_shape`=(1,)) and the model is predicting only one target variable (using a Dense layer with 1 unit). We are compiling the model with the mean squared error loss, using the `RMSprop` optimizer and training for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression via Sequential model\n",
    "# Train the deterministic linear model using mean squared error loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the model\n",
    "plt.scatter(x_train, y_train, alpha=0.4, label='data')\n",
    "plt.plot(x_train, model.predict(x_train), color='red', alpha=0.8, label='model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-investing",
   "metadata": {},
   "source": [
    "The deterministic linear regression fails to capture the aleatoric uncertainty (uncertainty on the process generation of the data). We can see this by the output above, where we get the y_value (our prediction) but we don't get any reference to the uncertainty of that prediction (there is significant uncertainty as we can see from the distance between the blue points and the red line)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-rebate",
   "metadata": {},
   "source": [
    "#### Probabilistic linear regression to model the aleatoric uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-trade",
   "metadata": {},
   "source": [
    "To build our probabilistic model, we add a final layer which is a `DistributionLambda` layer. This layer includes the normal distribution. Notice that the output of the previous layer is what defines the mean of the normal distribution and we are assuming that the standard deviation is fixed (in our case 0.6). The constructor of the `DistributionLambda` has one required argument which is a function. This function takes the output of the previous layer as an input and returns a distribution object. In this case we are using a lambda function to instantiate the `DistributionLambda` layer. The lambda function receives an input t, which is the output tensor of the previous `Dense` layer and returns a normal distribution with mean defined by the tensor t.\n",
    "\n",
    "With this setup the model returns a distribution object when it is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probabilistic regression with normal distribution as final layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-score",
   "metadata": {},
   "source": [
    "Notice that the output of the model is a distribution object with `batch_shape` equal to batch of the data (we did not create batches) by the number of units in the `Dense` layer (or number of ouput variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-fossil",
   "metadata": {},
   "source": [
    "It is time to define our loss function. The loss function in `Keras` receives the true labels and predictions as inputs. To compute the negative loglikelihood we just need to bear in mind that our model outputs a distribution object, so `y_pred` is the normal distribution. Using the `log_prob` method, we can easily compute the log probability of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using the negative loglikelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and a sample from the model\n",
    "\n",
    "y_model = model(x_train)\n",
    "y_sample = y_model.sample()\n",
    "y_hat = y_model.mean()\n",
    "y_sd = y_model.stddev()\n",
    "y_hat_m2sd = y_hat -2 * y_sd\n",
    "y_hat_p2sd = y_hat + 2*y_sd\n",
    "\n",
    "fig, (ax1, ax2) =plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
    "ax1.scatter(x_train, y_train, alpha=0.4, label='data')\n",
    "ax1.scatter(x_train, y_sample, alpha=0.4, color='red', label='model sample')\n",
    "ax1.legend()\n",
    "ax2.scatter(x_train, y_train, alpha=0.4, label='data')\n",
    "ax2.plot(x_train, y_hat, color='red', alpha=0.8, label='model $\\mu$')\n",
    "ax2.plot(x_train, y_hat_m2sd, color='green', alpha=0.8, label='model $\\mu \\pm 2 \\sigma$')\n",
    "ax2.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-thing",
   "metadata": {},
   "source": [
    "As we can see we capture the mean correctly but as we did not infer the standard deviation (it was manually chosen) we see that its value is far from correct. Let's add the standard deviation as a parameter of the model.\n",
    "\n",
    "We need to constraint the standard deviation to be positive. We use a SoftPlus function, which is a smooth approximation to the ReLU function and can indeed constrain the output of a model to always be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probabilistic regression with normal distribution as final layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using the negative loglikelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and a sample from the model\n",
    "\n",
    "y_model = model(x_train)\n",
    "y_sample = y_model.sample()\n",
    "y_hat = y_model.mean()\n",
    "y_sd = y_model.stddev()\n",
    "y_hat_m2sd = y_hat -2 * y_sd\n",
    "y_hat_p2sd = y_hat + 2*y_sd\n",
    "\n",
    "fig, (ax1, ax2) =plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
    "ax1.scatter(x_train, y_train, alpha=0.4, label='data')\n",
    "ax1.scatter(x_train, y_sample, alpha=0.4, color='red', label='model sample')\n",
    "ax1.legend()\n",
    "ax2.scatter(x_train, y_train, alpha=0.4, label='data')\n",
    "ax2.plot(x_train, y_hat, color='red', alpha=0.8, label='model $\\mu$')\n",
    "ax2.plot(x_train, y_hat_m2sd, color='green', alpha=0.8, label='model $\\mu \\pm 2 \\sigma$')\n",
    "ax2.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-cookbook",
   "metadata": {},
   "source": [
    "# 3. Non-Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-runner",
   "metadata": {},
   "source": [
    "#### Probabilitistic linear regression with nonlinear learned mean & variance\n",
    "\n",
    "Let's change the data to being nonlinear:\n",
    "\n",
    "$$y_i = x_i^3+\\frac{2}{5}(1+x_i)\\epsilon_i$$\n",
    "\n",
    "where $\\epsilon_i \\sim \\mathcal{N}(0,1)$ are independent and identically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and plot 10000 data points\n",
    "\n",
    "x_train = np.linspace(-1, 1, 1000)[:, np.newaxis]\n",
    "y_train = np.power(x_train, 3) + (2/5)*(1+x_train)*np.random.randn(1000)[:, np.newaxis]\n",
    "\n",
    "plt.scatter(x_train, y_train, alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-gather",
   "metadata": {},
   "source": [
    "To simplify the implementation of our last layer we can use a wrapper that TensorFlow Probability provides to build a similar distribution that we built with `DistributionLambda` - it is called `IndependentNormal`. At the same time we can use a static method that outputs the number of parameters that are required to the probabilistic layer and use it to define the number of units in the previous `Dense` layer: `tfpl.IndependentNormal.params_size(event_shape=1)`.\n",
    "\n",
    "The real difference between the linear and non-linear models is that we added a new Dense layer as the first layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probabilistic regression: normal distribution with fixed variance\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(input_shape=(1,), units=8, activation='sigmoid'),\n",
    "    Dense(tfpl.IndependentNormal.params_size(event_shape=1)),\n",
    "    tfpl.IndependentNormal(event_shape=1)\n",
    "    # Dense(2),\n",
    "    # tfpl.DistributionLambda(lambda t:tfd.Normal(loc=t[...,:1], scale=tf.math.softplus(t[...,1:])))\n",
    "])\n",
    "\n",
    "model.compile(loss=nll, optimizer=RMSprop(learning_rate=0.01))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "model.fit(x_train, y_train, epochs=500, verbose=False)\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and a sample from the model\n",
    "\n",
    "y_model = model(x_train)\n",
    "y_sample = y_model.sample()\n",
    "y_hat = y_model.mean()\n",
    "y_sd = y_model.stddev()\n",
    "y_hat_m2sd = y_hat -2 * y_sd\n",
    "y_hat_p2sd = y_hat + 2*y_sd\n",
    "\n",
    "fig, (ax1, ax2) =plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
    "ax1.scatter(x_train, y_train, alpha=0.4, label='data')\n",
    "ax1.scatter(x_train, y_sample, alpha=0.4, color='red', label='model sample')\n",
    "ax1.legend()\n",
    "ax2.scatter(x_train, y_train, alpha=0.4, label='data')\n",
    "ax2.plot(x_train, y_hat, color='red', alpha=0.8, label='model $\\mu$')\n",
    "ax2.plot(x_train, y_hat_m2sd, color='green', alpha=0.8, label='model $\\mu \\pm 2 \\sigma$')\n",
    "ax2.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-script",
   "metadata": {},
   "source": [
    "# 4. Deterministic Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-theorem",
   "metadata": {},
   "source": [
    "### 4.1 The MNIST dataset\n",
    "\n",
    "In this article we use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset and its corrupted version. The corrupted version have grey spatters on top of the numbers, make it harder to classify the numbers. Our goal is to build a Convolutional Neural Network (CNN) that classifies the images of the handwritten digits to 10 different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load training and testing data, with labels in integer and one-hot form\n",
    "\n",
    "def load_data(name):\n",
    "    data_dir = os.path.join('data', name)\n",
    "    x_train = 1 - np.load(os.path.join(data_dir, 'x_train.npy')) / 255.\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    y_train = np.load(os.path.join(data_dir, 'y_train.npy'))\n",
    "    y_train_oh = tf.keras.utils.to_categorical(y_train)\n",
    "    x_test  = 1 - np.load(os.path.join(data_dir, 'x_test.npy')) / 255.\n",
    "    x_test = x_test.astype(np.float32)\n",
    "    y_test  = np.load(os.path.join(data_dir, 'y_test.npy'))\n",
    "    y_test_oh = tf.keras.utils.to_categorical(y_test)\n",
    "    \n",
    "    return (x_train, y_train, y_train_oh), (x_test, y_test, y_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to inspect dataset digits\n",
    "\n",
    "def inspect_images(data, num_images):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=num_images, figsize=(2*num_images, 2))\n",
    "    for i in range(num_images):\n",
    "        ax[i].imshow(data[i, :, :], cmap='gray')\n",
    "        ax[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the MNIST dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "inspect_images(data=x_train, num_images=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "y_train = np.expand_dims(y_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "y_test = np.expand_dims(y_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the MNIST-C dataset\n",
    "\n",
    "(xy_c_train, xy_c_test) = tfds.load('mnist_corrupted/spatter',\n",
    "                               split=['train', 'test'],\n",
    "                               as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "fig, ax = plt.subplots(nrows=1, ncols=8, figsize=(2*8, 2))\n",
    "\n",
    "for x, y in xy_c_train:\n",
    "    if i < 8:\n",
    "        ax[i].imshow(np.squeeze(x), cmap='gray')\n",
    "    else:\n",
    "        break\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not good practice (doing it here to simplify visualizations essencially)\n",
    "\n",
    "x_c_train = []\n",
    "y_c_train = []\n",
    "for x, y in xy_c_train:\n",
    "    x_c_train.append(x)\n",
    "    y_c_train.append(y)\n",
    "x_c_train = np.asarray(x_c_train)\n",
    "y_c_train = np.asarray(y_c_train)\n",
    "\n",
    "\n",
    "x_c_test = []\n",
    "y_c_test = []\n",
    "for x, y in xy_c_test:\n",
    "    x_c_test.append(x)\n",
    "    y_c_test.append(y)\n",
    "x_c_test = np.asarray(x_c_test)\n",
    "y_c_test = np.asarray(y_c_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-causing",
   "metadata": {},
   "source": [
    "We need to build a classifier model that can output predictions to 10 different classes. In our deterministic model, the final layer has to be defined with a `Dense` layer with 10 units and a softmax activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-coordinator",
   "metadata": {},
   "source": [
    "We first define the deterministic model. It is a CNN classifier model with: \n",
    "* a Conv2D layer with 8 filters, 5x5 kernel size, ReLU activation and `'VALID'` padding.\n",
    "* a MaxPooling2D layer with a 6x6 window size.\n",
    "* a Flatten layer\n",
    "* a Dense layer with 10 units and softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that returns the deterministic model defined above\n",
    "\n",
    "def get_deterministic_model(input_shape, loss, optimizer, metrics):\n",
    "    \n",
    "    # model = \n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the benchmark model\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "deterministic_model = get_deterministic_model(\n",
    "    input_shape=(28, 28, 1), \n",
    "    loss=SparseCategoricalCrossentropy(), \n",
    "    optimizer=RMSprop(), \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "deterministic_model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "print('Accuracy on MNIST test set: ',\n",
    "      str(deterministic_model.evaluate(x_test, y_test, verbose=False)[1]))\n",
    "print('Accuracy on corrupted MNIST test set: ',\n",
    "      str(deterministic_model.evaluate(x_c_test, y_c_test, verbose=False)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-annex",
   "metadata": {},
   "source": [
    "The pointwise performance on the corrupted MNIST set is worse as we would expect. The dataset is noisier, which makes it hard to the model to differentiate the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-education",
   "metadata": {},
   "source": [
    "# 5. Probabilistic Deep Learning Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the negative loglikelihood function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-deficit",
   "metadata": {},
   "source": [
    "For our probabilistic model, the final layer is different from what we defined in the deterministic model. As we saw previously, we want our model to output a distribution object. In this case, the model outputs a One-Hot Categorical distribution object. With this approach we are able to model the aleatoric uncertainty on the image labels.\n",
    "\n",
    "OneHotCategorical is a discrete distribution over one-hot bit vectors whereas Categorical is a discrete distribution over positive integers. OneHotCategorical is equivalent to Categorical except Categorical has event_dim=() while OneHotCategorical has event_dim=K, where K is the number of classes.\n",
    "\n",
    "Our probabilistic CNN consists of: \n",
    "* a Conv2D layer with 8 filters, 5x5 kernel size, ReLU activation and `'VALID'` padding.\n",
    "* a MaxPooling2D layer with a 6x6 window size.\n",
    "* a Flatten layer\n",
    "* a Dense layer with the number of units required to parameterize the probabilistic layer that follows\n",
    "* a OneHotCategorical distribution with event shape equal to 10, corresponding to the 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the probabilistic CNN\n",
    "\n",
    "def get_probabilistic_model(input_shape, loss, optimizer, metrics):\n",
    "    \n",
    "    # model = \n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the probabilistic model\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "probabilistic_model = get_probabilistic_model(\n",
    "    input_shape=(28, 28, 1), \n",
    "    loss=nll, \n",
    "    optimizer=RMSprop(), \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "probabilistic_model.fit(x_train, tf.keras.utils.to_categorical(y_train), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "print('Accuracy on MNIST test set: ',\n",
    "      str(probabilistic_model.evaluate(x_test, tf.keras.utils.to_categorical(y_test), verbose=False)[1]))\n",
    "print('Accuracy on corrupted MNIST test set: ',\n",
    "      str(probabilistic_model.evaluate(x_c_test, tf.keras.utils.to_categorical(y_c_test), verbose=False)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-discipline",
   "metadata": {},
   "source": [
    "Note that the test accuracy of the probabilistic model is identical to the deterministic model. This is because the model architectures for both are equivalent; the only difference being that the probabilistic model returns a distribution object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all the weights of the deterministic and probabilistic \n",
    "# models are identical\n",
    "\n",
    "for deterministic_variable, probabilistic_variable in zip(deterministic_model.weights, probabilistic_model.weights):\n",
    "    print(np.allclose(deterministic_variable.numpy(), probabilistic_variable.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-orbit",
   "metadata": {},
   "source": [
    "# 6. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make plots of the probabilities that the model estimates for an image\n",
    "\n",
    "def analyse_model_prediction(data, true_labels, model, run_ensemble=False):\n",
    "    if run_ensemble:\n",
    "        ensemble_size = 200\n",
    "    else:\n",
    "        ensemble_size = 1\n",
    "    image = data\n",
    "    try:\n",
    "        if len(true_labels)>0:\n",
    "            true_labels = np.argmax(true_labels)\n",
    "    except:\n",
    "        pass\n",
    "    true_label = true_labels\n",
    "    predicted_probabilities = np.empty(shape=(ensemble_size, 10))\n",
    "    for i in range(ensemble_size):\n",
    "        predicted_probabilities[i] = model(image[np.newaxis, :]).mean().numpy()[0]\n",
    "    model_prediction = model(image[np.newaxis, :])\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 2),\n",
    "                                   gridspec_kw={'width_ratios': [2, 4]})\n",
    "    \n",
    "    # Show the image and the true label\n",
    "    ax1.imshow(image[..., 0], cmap='gray')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('True label: {}'.format(str(true_label)))\n",
    "    \n",
    "    # Show a 95% prediction interval of model predicted probabilities\n",
    "    pct_2p5 = np.array([np.percentile(predicted_probabilities[:, i], 2.5) for i in range(10)])\n",
    "    pct_97p5 = np.array([np.percentile(predicted_probabilities[:, i], 97.5) for i in range(10)])    \n",
    "    bar = ax2.bar(np.arange(10), pct_97p5, color='red')\n",
    "    bar[int(true_label)].set_color('green')\n",
    "    ax2.bar(np.arange(10), pct_2p5-0.02, color='white', linewidth=1, edgecolor='white')\n",
    "    ax2.set_xticks(np.arange(10))\n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2.set_ylabel('Probability')\n",
    "    ax2.set_title('Model estimated probabilities')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-mentor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prediction examples on MNIST\n",
    "\n",
    "for i in [0, 18]:\n",
    "    analyse_model_prediction(x_test[i], np.squeeze(y_test[i]), probabilistic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-handle",
   "metadata": {},
   "source": [
    "The model is very confident that the first image is a 7, which is correct. For the second image, the model struggles, assigning nonzero probabilities to many different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-blend",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prediction examples on MNIST-C\n",
    "\n",
    "for i in [0, 17]:\n",
    "    analyse_model_prediction(x_c_test[i], np.squeeze(y_c_test[i]), probabilistic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-technology",
   "metadata": {},
   "source": [
    "Once again the model is confident about its prediction of the first image. Despite the spatters, the number is still easy to identify. The second number is significantly harder to identify. The model still does a good job by predicting the right number, but also by showing uncertainty about that choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-winner",
   "metadata": {},
   "source": [
    "We can also make some analysis of the model's uncertainty across the full test set, instead of for individual values. One way to do this is to calculate the entropy of the distribution. The entropy is the expected information of a random variable, and is a measure of the uncertainty of the random variable. The entropy of the estimated probabilities for sample  𝑖  is defined as\n",
    "\n",
    "$$\n",
    "H_i = -\\sum_{j=1}^{10} p_{ij} \\text{log}_{2}(p_{ij})\n",
    "$$\n",
    " \n",
    "where  𝑝𝑖𝑗  is the probability that the model assigns to sample  𝑖  corresponding to label  𝑗. The higher the value, the more unsure the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to plot the distribution of the information entropy across samples,\n",
    "# split into whether the model prediction is correct or incorrect\n",
    "\n",
    "def get_correct_indices(model, x, labels):\n",
    "    y_model = model(x)\n",
    "    correct = np.argmax(y_model.mean(), axis=1) == np.squeeze(labels)\n",
    "    correct_indices = [i for i in range(x.shape[0]) if correct[i]]\n",
    "    incorrect_indices = [i for i in range(x.shape[0]) if not correct[i]]\n",
    "    return correct_indices, incorrect_indices\n",
    "\n",
    "\n",
    "def plot_entropy_distribution(model, x, labels):\n",
    "    probs = model(x).mean().numpy()\n",
    "    entropy = -np.sum(probs * np.log2(probs), axis=1)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    for i, category in zip(range(2), ['Correct', 'Incorrect']):\n",
    "        entropy_category = entropy[get_correct_indices(model, x, labels)[i]]\n",
    "        mean_entropy = np.mean(entropy_category)\n",
    "        num_samples = entropy_category.shape[0]\n",
    "        title = category + 'ly labelled ({:.1f}% of total)'.format(num_samples / x.shape[0] * 100)\n",
    "        axes[i].hist(entropy_category, weights=(1/num_samples)*np.ones(num_samples))\n",
    "        axes[i].annotate('Mean: {:.3f} bits'.format(mean_entropy), (0.4, 0.9), ha='center')\n",
    "        axes[i].set_xlabel('Entropy (bits)')\n",
    "        axes[i].set_ylim([0, 1])\n",
    "        axes[i].set_ylabel('Probability')\n",
    "        axes[i].set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy plots for the MNIST dataset\n",
    "\n",
    "print('MNIST test set:')\n",
    "plot_entropy_distribution(probabilistic_model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy plots for the MNIST-C dataset\n",
    "\n",
    "print('Corrupted MNIST test set:')\n",
    "plot_entropy_distribution(probabilistic_model, x_c_test, y_c_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-profit",
   "metadata": {},
   "source": [
    "# 7. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-shelter",
   "metadata": {},
   "source": [
    "From the results above we can see that the model is more unsure on the predictions it got wrong this means it knows when the prediction may be wrong. These are great properties to have in a machine learning model, and is one of the advantages of probabilistic modelling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
